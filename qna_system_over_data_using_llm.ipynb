{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [QnA System Over Structured Data using LLM](#toc1_)    \n",
    "  - [Introduction](#toc1_1_)    \n",
    "  - [Theoritical Basis](#toc1_2_)    \n",
    "    - [Generative AI](#toc1_2_1_)    \n",
    "    - [Large Language Models](#toc1_2_2_)    \n",
    "      - [LLM Development vs. Traditional Development](#toc1_2_2_1_)    \n",
    "      - [LLM usage in various business contexts](#toc1_2_2_2_)    \n",
    "    - [Langchain](#toc1_2_3_)    \n",
    "  - [Environment Set-up](#toc1_3_)    \n",
    "      - [Mengatur Key API dan File `.env`](#toc1_3_1_1_)    \n",
    "  - [Langchain Quick Start](#toc1_4_)    \n",
    "    - [Prompt](#toc1_4_1_)    \n",
    "      - [Basic Prompt](#toc1_4_1_1_)    \n",
    "    - [Prompt Template](#toc1_4_2_)    \n",
    "    - [Agent](#toc1_4_3_)    \n",
    "  - [Build Question Aswering System](#toc1_5_)    \n",
    "    - [SQLite Database](#toc1_5_1_)    \n",
    "      - [Basic Question Answer System](#toc1_5_1_1_)    \n",
    "    - [Structured Data](#toc1_5_2_)    \n",
    "    - [Unstructured Data](#toc1_5_3_)    \n",
    "      - [Visualization](#toc1_5_3_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[QnA System Over Structured Data using LLM](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='toc1_1_'></a>[Introduction](#toc0_)\n",
    "\n",
    "Latar belakang proyek ini berasal dari kebutuhan akan solusi yang lebih efisien dan efektif dalam menjawab pertanyaan yang berkaitan dengan data terstruktur. Dalam lingkungan bisnis yang semakin kompleks, akses cepat dan akurat terhadap informasi yang tersembunyi dalam data terstruktur menjadi krusial. Namun, pengolahan data semacam ini dapat menjadi rumit dan memerlukan upaya manual yang signifikan. Oleh karena itu, mengintegrasikan kecerdasan buatan, seperti Large Language Model (LLM), dengan data terstruktur dapat membawa solusi yang inovatif untuk meningkatkan efisiensi dan kualitas dalam menjawab pertanyaan-pertanyaan tersebut.\n",
    "\n",
    "> ðŸ”Ž Tujuan utama dari proyek ini adalah mengembangkan sebuah sistem pertanyaan dan jawaban yang menggabungkan kekuatan Large Language Model (LLM) dengan informasi terstruktur dari data dalam format terstruktur, seperti file CSV atau SQL. \n",
    "\n",
    "Dengan memadukan potensi LLM dalam memahami bahasa alami dengan informasi terstruktur yang ada dalam dataset, proyek ini bertujuan untuk menciptakan solusi yang dapat memberikan jawaban yang lebih kontekstual dan akurat. Selain itu, proyek ini juga bermaksud untuk mengeksplorasi cara-cara mengoptimalkan interaksi antara data teks dan terstruktur, dengan harapan dapat memberikan panduan yang berguna bagi pengembangan solusi serupa di masa depan. Melalui kombinasi ini, proyek ini diharapkan mampu menyediakan alat yang bermanfaat untuk meningkatkan efisiensi, produktivitas, dan pengambilan keputusan dalam berbagai konteks bisnis yang mengandalkan analisis data terstruktur.\n",
    "\n",
    "<img title=\"karate cat\" src=\"assets/GIF_Generative-ChatGPT.gif\" width = 60%>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Theoritical Basis](#toc0_)\n",
    "\n",
    "### <a id='toc1_2_1_'></a>[Generative AI](#toc0_)\n",
    "\n",
    "<img title=\"karate cat\" src=\"assets/AI-GIF-Generator-min.gif\" width = 60%>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <p>\n",
    "    <li>Generative AI adalah subbidang kecerdasan buatan yang berfokus pada pengembangan model yang mampu menghasilkan data baru yang sebelumnya tidak ada.</li>\n",
    "    <li>Model generatif dapat menciptakan teks, gambar, suara, dan data lainnya yang serupa dengan data yang digunakan untuk melatih model.</li>\n",
    "    <li>Model generative AI, seperti Large Language Models (LLM), adalah contoh utama dari jenis ini. Model-model ini dilatih pada jumlah besar data dan dapat menghasilkan teks, gambar, suara, dan bahkan video yang seolah-olah dibuat oleh manusia.</li>\n",
    "    <hr><br>\n",
    "    ðŸ“Œ <b>Pada intinya, generative AI memiliki kemampuan untuk \"mengerti\" pola dan struktur dari data pelatihan dan menggunakan pemahaman ini untuk membuat data baru yang serupa. </b>\n",
    "    </p>\n",
    "</div> \n",
    "\n",
    "Konsep generative AI memainkan peran sentral dalam proyek ini yang berfokus pada pengembangan sistem Pertanyaan dan Jawaban (QA) berbasis Large Language Model (LLM) menggunakan data terstruktur. \n",
    "\n",
    "> ðŸ”Ž Generative AI merupakan paradigma yang memungkinkan penciptaan data baru yang otentik, dengan model yang mampu menghasilkan informasi yang sebelumnya tidak ada berdasarkan pemahaman pola dari data pelatihan. \n",
    "\n",
    "Dalam konteks proyek ini, generative AI membuka pintu untuk **menghasilkan jawaban yang kontekstual** dan relevan dalam sistem QA. Dengan memanfaatkan Large Language Model, yang pada dasarnya merupakan jenis generative AI, sistem ini dapat menghasilkan jawaban berdasarkan pemahaman bahasa alami serta informasi terstruktur yang ditemukan dalam data yang disajikan dalam format terstruktur. Integrasi generative AI dalam solusi QA ini mampu menghasilkan respons yang lebih kreatif dan kontekstual, serta memberikan solusi yang potensial dalam meningkatkan efektivitas komunikasi dan analisis dalam berbagai konteks bisnis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_2_'></a>[Large Language Models](#toc0_)\n",
    "\n",
    "LLM adalah aplikasi khusus dari Generative AI yang difokuskan pada generasi teks. Mereka menggunakan teknik generatif AI, seperti pemodelan probabilitas dan prediksi urutan, untuk menghasilkan teks yang menyerupai manusia. Dengan memodelkan sifat statistik bahasa, LLM dapat menghasilkan teks yang kohesif, relevan, dan sering kali sulit dibedakan dari teks yang ditulis oleh manusia.\n",
    "\n",
    "Istilah \"Large\" dalam konteks Large Language Models (LLM) merujuk pada dua aspek:\n",
    "\n",
    "**1. Dataset pelatihan besar**\n",
    "\n",
    " - LLM umumnya dilatih pada jumlah besar data teks untuk menangkap pola dan struktur statistik yang ada dalam bahasa.\n",
    " - Dataset pelatihan dapat terdiri dari berbagai sumber seperti buku, artikel, situs web, dan korpus teks lainnya, yang memberikan beragam pola dan konteks linguistik.\n",
    " - Dengan menggunakan dataset pelatihan yang besar, LLM memiliki kesempatan untuk belajar dari sejumlah besar informasi dan meningkatkan kemampuan pemodelan bahasanya.\n",
    "    \n",
    "**2. Jumlah parameter besar**\n",
    "\n",
    " - LLM ditandai dengan jumlah parameter yang signifikan, yaitu variabel yang dapat dipelajari yang menentukan perilaku model.\n",
    " - Jumlah parameter dalam LLM dapat berkisar dari jutaan hingga miliaran, tergantung pada ukuran dan kompleksitas model.\n",
    " - Meningkatkan jumlah parameter memungkinkan LLM untuk menangkap pola bahasa yang lebih rumit dan menghasilkan teks yang lebih kohesif dan kontekstual.\n",
    " - Model dengan jumlah parameter yang lebih besar dapat menangani berbagai tugas bahasa dan menunjukkan kemampuan generasi bahasa yang lebih unggul.\n",
    "\n",
    "\n",
    "LLM adalah model canggih yang dirancang untuk berbagai tugas yang terkait dengan teks. Tugas-tugas ini meliputi:\n",
    "\n",
    "<img title=\"llm problem\" src=\"assets/llm_problem.png\">\n",
    "\n",
    "1. **Klasifikasi Teks**: LLM dapat diadaptasi untuk mengklasifikasikan teks ke dalam berbagai kategori atau label yang telah ditentukan sebelumnya. Melalui pelatihan dengan data yang telah diberi label, mereka belajar untuk mengidentifikasi pola-pola yang mengindikasikan kelas-kelas tertentu. Contohnya termasuk mengklasifikasikan email sebagai spam atau bukan, menganalisis sentimen (positif, negatif, netral) dalam teks, dan mengategorikan artikel berita berdasarkan topiknya.\n",
    "\n",
    "2. **Pertanyaan dan Jawaban**: LLM dapat memahami pertanyaan dan memberikan jawaban yang akurat berdasarkan konteks atau basis pengetahuan yang diberikan. Melalui pelatihan dengan pasangan pertanyaan dan jawaban, mereka dapat menghasilkan respons yang relevan. Mereka digunakan dalam chatbot, asisten virtual, dan mesin pencari.\n",
    "\n",
    "3. **Ringkasan Dokumen**: LLM mampu merangkum dokumen-dokumen panjang dengan mengidentifikasi informasi-informasi penting. Melalui pelatihan dengan pasangan dokumen dan ringkasannya, mereka belajar untuk membuat ringkasan yang koheren dan singkat. Ini berguna dalam pengambilan informasi, analisis konten, dan kompresi data.\n",
    "\n",
    "4. **Penghasilan Teks**: LLM memiliki kemampuan untuk menghasilkan teks yang menyerupai bahasa manusia untuk berbagai tujuan. Mereka memanfaatkan pemahaman dan kemampuan generasi bahasa mereka untuk menghasilkan teks yang koheren dan sesuai konteks. Mereka dapat disesuaikan untuk membuat cerita, puisi, deskripsi produk, dan lainnya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_2_1_'></a>[LLM Development vs. Traditional Development](#toc0_)\n",
    "\n",
    "| Pengembangan LLM (menggunakan API yang telah dipre-train) | Pengembangan ML Tradisional |\n",
    "| ----------- | ----------- |\n",
    "| Tidak memerlukan keahlian ML      | Memerlukan keahlian ML       |\n",
    "| Tidak memerlukan contoh pelatihan   | Memerlukan contoh pelatihan       |\n",
    "| Tidak perlu melatih model | Perlu melatih model|\n",
    "| Memikirkan desain permintaan | Memikirkan cara meminimalkan fungsi kerugian|\n",
    "\n",
    "\n",
    "#### <a id='toc1_2_2_2_'></a>[LLM usage in various business contexts](#toc0_)\n",
    "\n",
    "**1. Customer Support and Chatbots:**\n",
    "   - Showcasing a chatbot powered by an LLM that can handle customer inquiries, provide product recommendations, and assist with common support issues.\n",
    "   - Demonstrating the chatbot's ability to understand and respond to user queries, maintaining a natural and conversational interaction.\n",
    "\n",
    "**2. Content Generation and Marketing:**\n",
    "   - Demonstrating how an LLM can generate engaging and personalized content for marketing campaigns, social media posts, or email newsletters.\n",
    "   - Showcasing the LLM's ability to generate content that aligns with the brand's voice and resonates with the target audience.\n",
    "\n",
    "**3. Data Analysis and Insights:**\n",
    "   - Demonstrating how an LLM can analyze large volumes of customer feedback, reviews, or survey responses to extract valuable insights.\n",
    "   - Showcasing the LLM's ability to identify trends, sentiments, and key topics within the data, enabling data-driven decision-making.\n",
    "\n",
    "**4. Document Processing and Automation:**\n",
    "   - Demonstrating how an LLM can automate the processing of documents, such as contract analysis, by extracting relevant information, identifying clauses, or generating summaries.\n",
    "   - Showcasing the time-saving and accuracy improvements achieved through LLM-powered document automation.\n",
    "\n",
    "**5. Language Translation and Localization:**\n",
    "   - Demonstrating an LLM's ability to perform accurate language translation across multiple languages.\n",
    "   - Showcasing how the LLM can handle nuances, idioms, and context-specific translations, enabling effective communication in global markets.\n",
    "\n",
    "**6. Data-driven Decision-making:**\n",
    "   - Demonstrating how an LLM can analyze market trends, customer behavior, or social media data to provide insights for strategic decision-making.\n",
    "   - Showcasing the LLM's ability to identify patterns, correlations, or emerging topics within the data, facilitating informed business decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <p>\n",
    "    <b>Large Language Model (LLM)</b>\n",
    "    <li>Large Language Models (LLMs) adalah model bahasa yang diperluas, seperti GPT (Generative Pre-trained Transformer) dan BERT (Bidirectional Encoder Representations from Transformers).</li>\n",
    "    <li>LLMs dapat memahami dan menghasilkan teks yang kompleks dengan mempelajari pola bahasa dari data pelatihan yang besar dan beragam.</li>\n",
    "    <li>Mereka menggunakan arsitektur Transformer, yang memiliki kemampuan untuk memahami konteks dalam teks dengan cara yang lebih baik dibandingkan arsitektur sebelumnya.</li>\n",
    "    <hr><br>\n",
    "    <b>Understanding Transformer</b>\n",
    "    <li>Transformer adalah arsitektur model yang mendasari banyaknya kemajuan dalam bidang pemrosesan bahasa alami.</li>\n",
    "    <li>Ini mengatasi masalah dependensi jarak jauh dalam bahasa dengan menggunakan mekanisme self-attention, yang memungkinkan model untuk mengaitkan kata-kata yang berjauhan dalam teks.</li>\n",
    "    <li>Transformer terdiri dari encoder dan decoder, dengan encoder bertanggung jawab untuk memahami teks asli dan decoder menghasilkan teks hasil terjemahan atau generasi.</li>\n",
    "    <hr><br>\n",
    "    <b>Popular Large Language Models</b>\n",
    "    <li>GPT-3 (Generative Pre-trained Transformer 3) adalah salah satu LLM paling terkenal yang dikembangkan oleh OpenAI. Ini memiliki 175 miliar parameter dan mampu melakukan berbagai tugas bahasa alami.</li>\n",
    "    <li>BERT (Bidirectional Encoder Representations from Transformers) adalah LLM yang fokus pada pemahaman konteks dengan memeriksa konteks sebelum dan sesudah setiap kata dalam kalimat.</li>\n",
    "    <li>T5 (Text-to-Text Transfer Transformer) adalah LLM yang mengusulkan pendekatan \"teks-ke-teks\" untuk berbagai tugas bahasa, seperti menerjemahkan, menyusun, dan banyak lagi.</li>\n",
    "    <hr><br>\n",
    "    ðŸ“Œ <b>Konsep LLMs adalah inti dalam proyek ini, dengan menggabungkan informasi dari data terstruktur, seperti file CSV, dengan pemahaman LLM tentang bahasa, sistem QA yang dihasilkan memberikan jawaban lebih kontekstual dan akurat.</b>\n",
    "    </p>\n",
    "</div> \n",
    "\n",
    "\n",
    "\n",
    "    <b></b>\n",
    "    <li></li>\n",
    "    <li></li>\n",
    "    <li></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_3_'></a>[Langchain](#toc0_)\n",
    "\n",
    "[LangChain](https://python.langchain.com/docs/get_started/introduction.html) merupakan alat yang relevan dalam konteks proyek ini yang fokus pada pengembangan sistem Pertanyaan dan Jawaban (QA) menggunakan Large Language Model (LLM) dan data terstruktur. LangChain adalah *framework* Python yang dirancang khusus untuk mengintegrasikan informasi dari data terstruktur, seperti file CSV, dengan kekuatan pemrosesan bahasa alami yang dimiliki oleh LLM. Dengan menggunakan LangChain, proyek ini memiliki kemampuan untuk menghubungkan antara konteks terstruktur dalam data dengan pemahaman bahasa alami, sehingga memungkinkan sistem QA untuk memberikan jawaban yang lebih kontekstual dan akurat. Melalui penggunaan pustaka ini, integrasi antara data terstruktur dan LLM dapat dilakukan dengan lebih efisien dan efektif, membantu menciptakan solusi QA yang lebih adaptif dan canggih."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Environment Set-up](#toc0_)\n",
    "\n",
    "\n",
    "Penggunaan LangChain biasanya memerlukan integrasi dengan satu atau lebih penyedia model, penyimpanan data, API, dan sebagainya. Pada contoh ini, kami akan menggunakan API model dari OpenAI.\n",
    "\n",
    "#### <a id='toc1_3_1_1_'></a>[Mengatur Key API dan File `.env`](#toc0_)\n",
    "\n",
    "Akses ke API memerlukan Key API, yang dapat Anda peroleh dengan membuat akun dan mengunjungi halaman ini. Ketika mengatur Key API dan menggunakan file .env dalam proyek Python Anda, ikuti langkah umum berikut:\n",
    "\n",
    "1. **Dapatkan Key API**: Jika Anda bekerja dengan API atau layanan eksternal yang memerlukan Key API, Anda perlu memperolehnya dari penyedia. Ini biasanya melibatkan pendaftaran akun dan pembuatan Key API khusus untuk proyek Anda.\n",
    "\n",
    "2. **Buat file .env**: Di direktori proyek Anda, buat berkas baru dan beri nama \".env\". Berkas ini akan menyimpan kunci API dan informasi sensitif lainnya secara aman.\n",
    "\n",
    "3. **Simpan kunci API di .env**: Buka berkas .env dengan editor teks dan tambahkan baris untuk menyimpan kunci API Anda. Formatnya harus `OPENAI_API_KEY=key_api_anda`, di mana \"OPENAI_API_KEY\" adalah nama variabel dan \"key_api_anda\" adalah nilai sebenarnya dari key API Anda. Pastikan untuk tidak menyertakan tanda kutip atau spasi di sekitar nilai tersebut.\n",
    "\n",
    "4. **Muat variabel lingkungan**: Dalam kode Python Anda, Anda perlu memuat variabel lingkungan dari berkas .env sebelum mengaksesnya. Impor modul dotenv dan tambahkan kode berikut di awal skrip Anda:\n",
    "\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Muat variabel lingkungan dari berkas .env\n",
    "load_dotenv()\n",
    "```\n",
    "\n",
    "> `dotenv` adalah pustaka Python yang populer yang menyederhanakan proses memuat variabel lingkungan dari berkas .env ke aplikasi Python Anda. Ini memungkinkan Anda menyimpan variabel konfigurasi secara terpisah dari kode Anda, sehingga lebih mudah mengelola informasi sensitif seperti kunci API, kredensial basis data, atau pengaturan khusus lingkungan lainnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Langchain Quick Start](#toc0_)\n",
    "\n",
    "Dalam `LangChain`, Panduan Cepat melibatkan tiga komponen utama: Prompt, Chain, dan Agen.\n",
    "\n",
    "Dengan komponen **Prompt, Chain, dan Agen** yang bekerja bersama-sama, kita dapat terlibat dalam percakapan interaktif dengan model bahasa. Prompt menetapkan konteks atau memulai percakapan, Rantai mengelola riwayat percakapan, dan Agen mengelola komunikasi antara pengguna dan model bahasa.\n",
    "\n",
    "Dengan menggunakan komponen-komponen ini, kita dapat membangun aplikasi yang dinamis dan **interaktif** yang melibatkan interaksi bolak-balik dengan model bahasa, memungkinkan kita untuk membuat **agen percakapan**, **chatbot**, **sistem tanya jawab**, dan banyak lagi.\n",
    "\n",
    "Untuk berinteraksi dengan pustaka `LangChain` menggunakan model bahasa dari OpenAI, kita perlu:\n",
    "\n",
    "1. **Mengimpor Modul yang Diperlukan**: Kode mengimpor pustaka LangChain dengan pernyataan `from langchain import OpenAI`.\n",
    "\n",
    "2. **Membuat Instansi OpenAI**: Kode membuat instansi kelas `OpenAI` dan mengassignnya ke variabel `llm`. Instansi ini mewakili koneksi ke model bahasa OpenAI.\n",
    "\n",
    "3. **Mengatur Parameter Suhu (Temperature)**: Parameter `temperature` dimasukkan ke `OpenAI` saat inisialisasinya. Suhu (temperature) adalah parameter yang mengendalikan tingkat acak keluaran dari model bahasa.\n",
    "\n",
    "> Nilai suhu yang lebih tinggi (misalnya, 0.9) membuat teks yang dihasilkan lebih **beragam dan kreatif**, sementara nilai yang lebih rendah (misalnya, 0.2) membuatnya lebih **terfokus dan deterministik**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_1_'></a>[Prompt](#toc0_)\n",
    "\n",
    "#### <a id='toc1_4_1_1_'></a>[Basic Prompt](#toc0_)\n",
    "\n",
    "**Prompt** mengacu pada masukan atau instruksi awal yang diberikan kepada model bahasa untuk menghasilkan respons. Ini menetapkan konteks dan memberikan panduan bagi model bahasa untuk menghasilkan teks yang relevan dan koheren.\n",
    "\n",
    "Dalam contoh ini, prompt meminta saran untuk nama yang bagus untuk merek yang mengkhususkan diri dalam burger lokal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The BurgerHQ.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is a good name for a brand that makes local burger?\"\n",
    "print(llm.predict(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model bahasa kemudian menggunakan pengetahuannya dan pelatihannya untuk menghasilkan respons yang sesuai dengan prompt yang diberikan. Perhatikan bahwa setiap kali dijalankan, model akan menghasilkan jawaban yang baru.\n",
    "\n",
    "> Fungsi `llm.predict()` dipanggil dengan menggunakan prompt sebagai masukan. Fungsi ini mengirimkan prompt ke model bahasa dan **menghasilkan** sebuah respons berdasarkan masukan yang diberikan. Teks yang dihasilkan mewakili **prediksi model bahasa** atau **kelengkapan** dari prompt.\n",
    "\n",
    "Kita juga dapat melakukannya dalam bahasa lain, mari coba dengan Bahasa Indonesia.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mentai Pisang.\n"
     ]
    }
   ],
   "source": [
    "print(llm.predict(\"Nama yang bagus untuk brand yang membuat pisang goreng mentai?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dengan hanya memberikan prompt dalam Bahasa Indonesia, kita dapat memperoleh respons berupa teks yang dihasilkan dalam Bahasa Indonesia juga. Ini menunjukkan kecanggihan model bahasa seperti LangChain dalam memahami dan menghasilkan teks dalam berbagai bahasa, memungkinkan untuk aplikasi dan interaksi multibahasa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "### <a id='toc1_4_2_'></a>[Prompt Template](#toc0_)\n",
    "\n",
    "Aplikasi LLM umumnya menggunakan templat prompt alih-alih langsung memasukkan pertanyaan pengguna ke dalam LLM. Pendekatan ini melibatkan penyertaan input pengguna ke dalam konteks teks yang lebih besar yang dikenal sebagai templat prompt.\n",
    "\n",
    "**Templat prompt** adalah format terstruktur yang dirancang untuk menghasilkan prompt secara konsisten. Ini terdiri dari serangkaian string teks, yang disebut \"templat,\" yang dapat menggabungkan berbagai parameter yang diberikan oleh pengguna akhir untuk membuat prompt yang dinamis.\n",
    "\n",
    "Tema templat prompt dapat mencakup:\n",
    "\n",
    "- Instruksi untuk memandu respons model bahasa.\n",
    "- Beberapa contoh \"few-shot\" untuk membantu model bahasa menghasilkan keluaran yang lebih akurat dan sesuai konteks.\n",
    "- Pertanyaan yang diajukan kepada model bahasa.\n",
    "\n",
    "Pada contoh sebelumnya, teks yang diberikan ke model berisi instruksi untuk menghasilkan nama merek berdasarkan deskripsi yang diberikan. Pada aplikasi kita, akan lebih nyaman bagi pengguna untuk hanya memberikan deskripsi perusahaan atau produk tanpa perlu secara eksplisit memberikan instruksi kepada model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Membuat** templat prompt: Gunakan metode `PromptTemplate.from_template()` untuk membuat objek `PromptTemplate` dari string templat.\n",
    "\n",
    "Dalam kasus ini, string templatnya adalah \"Apa nama yang bagus untuk merek yang membuat {produk}?\", di mana `{produk}` berfungsi sebagai placeholder untuk nama produk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template\n",
    "template_prompt = PromptTemplate.from_template(\"What is a good name for a brand that makes {product}?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mengatur format** templat prompt: Gunakan metode `.format()` dari objek `PromptTemplate` untuk mengganti placeholder dalam templat dengan nilai yang diinginkan. Dalam kasus ini, placeholder `{produk}` diganti dengan string \"local burger\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for a brand that makes local burger?\n"
     ]
    }
   ],
   "source": [
    "# Format the prompt template\n",
    "prompt = template_prompt.format(product=\"local burger\")\n",
    "\n",
    "# Print the prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Local Burger Co.\n"
     ]
    }
   ],
   "source": [
    "print(llm.predict(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karena yang sudah kita buat merupakan sebuah templat, kita dapat memasukkan lebih dari 1 input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines a string template for a poem\n",
    "template = \"Write a {adjective} poem about {subject}\"\n",
    "\n",
    "# creates a prompt template\n",
    "poem_template = PromptTemplate(\n",
    "    input_variables=[\"adjective\", \"subject\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a sad poem about ducks'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem_template.format(adjective='sad', subject='ducks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mengatur format dari templat dengan mengganti placeholder `{adjective}` dan `{subject}` dengan nilai yang disediakan. Hasil stringnya akan menjadi \"Write a sad poem about ducks\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Eight little ducks, waddling and quacking\n",
      "Doing their best not to be flapping\n",
      "Singing merrily in the park\n",
      "But deep down thereâ€™s a fear they mark\n",
      "\n",
      "Their pond has dried up, their habitat gone\n",
      "Their friends and family, they are now alone\n",
      "What will become of these helpless ducks\n",
      "As the sun glares down on their luck\n",
      "\n",
      "The silly ducks looked around in dismay\n",
      "There was no water, and no place to stay\n",
      "They didnâ€™t know what to do but cry\n",
      "What would tomorrow bring, and why?\n",
      "\n",
      "The ducks all huddled together in fear\n",
      "Wondering how long the dry spell would appear\n",
      "Their quacks seemed to echo through the land\n",
      "A sad reminder of their once safe sand\n"
     ]
    }
   ],
   "source": [
    "# generate a response\n",
    "print(llm.predict(poem_template.format(adjective='sad', subject='ducks')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kita dapat membuat templat cepat yang berfungsi sebagai konsultan penamaan untuk perusahaan baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BatikBay.\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt template\n",
    "template = \"\"\"\n",
    "I want you to act as a naming consultant for new companies.\n",
    "\n",
    "Here are some examples of good company names:\n",
    "\n",
    "- search engine, Google\n",
    "- social media, Facebook\n",
    "- video sharing, YouTube\n",
    "\n",
    "The name should be short, catchy and easy to remember.\n",
    "\n",
    "What is a good name for a brand that makes {product}?\n",
    "\"\"\"\n",
    "\n",
    "# Create a PromptTemplate object\n",
    "brand_template = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "# Format the prompt template with specific industry values\n",
    "batik_prompt = brand_template.format(product='batik')\n",
    "\n",
    "# Print the formatted prompt\n",
    "print(llm.predict(batik_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_3_'></a>[Agent](#toc0_)\n",
    "\n",
    "Dalam alur kerja yang lebih kompleks, menjadi penting untuk memiliki kemampuan membuat keputusan dan memilih tindakan berdasarkan konteks yang diberikan. Inilah tempat peran agen menjadi relevan.\n",
    "\n",
    "Agen memanfaatkan model bahasa untuk **menentukan tindakan apa** yang harus diambil dan dalam urutan apa. Mereka memiliki kumpulan alat yang tersedia, dan terus memilih, melaksanakan, dan mengevaluasi alat-alat ini sampai mereka mencapai solusi optimal. Agen memberikan pendekatan dinamis dan adaptif untuk **memecahkan masalah** dalam kerangka kerja LangChain, yang memungkinkan alur kerja yang lebih canggih dan fleksibel.\n",
    "\n",
    "Untuk memuat agen dalam LangChain, Anda perlu mempertimbangkan komponen-komponen berikut:\n",
    "\n",
    "- **LLM/Model Chat:** Ini merujuk pada model bahasa yang menggerakkan agen. Model ini bertanggung jawab untuk menghasilkan respons berdasarkan masukan yang diberikan. Anda dapat memilih dari berbagai model yang telah dilatih sebelumnya atau menggunakan model kustom Anda sendiri.\n",
    "\n",
    "- **Alat-Alat:** Alat-alat adalah fungsi atau metode yang melakukan tugas-tugas tertentu dalam alur kerja agen. Ini bisa mencakup tindakan seperti Pencarian Google, pencarian basis data, Python REPL (Read-Eval-Print Loop), atau bahkan rantai-rantai lain. LangChain menyediakan kumpulan alat bawaan dengan spesifikasinya sendiri, yang dapat Anda lihat di dokumentasi Alat-Alat.\n",
    "\n",
    "- **Nama Agen:** Nama agen adalah string yang mengidentifikasi kelas agen yang didukung. Setiap kelas agen diberi parameter oleh prompt yang digunakan oleh model bahasa untuk menentukan tindakan yang sesuai untuk diambil. Dalam konteks ini, kita akan fokus pada penggunaan agen yang didukung secara standar, daripada mengimplementasikan agen kustom. Anda dapat menjelajahi daftar agen yang didukung dan spesifikasinya untuk memilih yang paling cocok untuk aplikasi Anda.\n",
    "\n",
    "Untuk contoh tertentu yang disebutkan, kita akan memanfaatkan alat `wikipedia` untuk mengambil dan mendapatkan respons berdasarkan informasi dari Wikipedia. Alat ini memungkinkan agen mengakses informasi relevan dari Wikipedia dan memberikan respons informatif berdasarkan masukan yang diberikan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Tentukan model bahasa untuk agen**: Dalam contoh ini, `llm_agent` diinisialisasi dengan kelas `OpenAI`, yang mewakili model bahasa. Parameter `temperature` menentukan tingkat acak dalam respons yang dihasilkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The language model we're going to use to control the agent.\n",
    "llm_agent = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the tools**: Fungsi `load_tools` digunakan untuk memuat alat yang diinginkan untuk agen. Dalam hal ini, alat \"wikipedia\" dan \"llm-math\" dimuat. \n",
    "\n",
    "> \"wikipedia\" memungkinkan agen mengakses informasi dari Wikipedia, sedangkan alat \"llm-math\" menggunakan model bahasa untuk operasi matematika."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inisiasi Agent** -> Fungsi `initialize_agent` dipanggil untuk membuat instance agen. Dibutuhkan alat yang dimuat, model bahasa (`llm_agent`), jenis agen (`AgentType.ZERO_SHOT_REACT_DESCRIPTION`), dan parameter `verbose` opsional. Tipe agen menentukan perilaku agen, seperti menghasilkan respons berdasarkan deskripsi atau bereaksi terhadap masukan pengguna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm_agent, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I need to find out the average salary of data scientists in America and then convert it to Indonesia Rupiah.\n",
      "Action: Wikipedia\n",
      "Action Input: Average salary of data scientists in America\u001b[0m"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m     httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    462\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m     \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m     response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1376\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m \u001b[39mexcept\u001b[39;00m timeout:\n",
      "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 54] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/urllib3/connectionpool.py:798\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    796\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 798\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    799\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    801\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/urllib3/util/retry.py:550\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[39mif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 550\u001b[0m     \u001b[39mraise\u001b[39;00m six\u001b[39m.\u001b[39;49mreraise(\u001b[39mtype\u001b[39;49m(error), error, _stacktrace)\n\u001b[1;32m    551\u001b[0m \u001b[39melif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/urllib3/packages/six.py:769\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[0;32m--> 769\u001b[0m     \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    770\u001b[0m \u001b[39mraise\u001b[39;00m value\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m     httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    462\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m     \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m     response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1376\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m \u001b[39mexcept\u001b[39;00m timeout:\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agent\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mHow much do data scientists make in America? How much if converted into Indonesia Rupiah?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/chains/base.py:290\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, *args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    289\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 290\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags)[_output_key]\n\u001b[1;32m    292\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags)[_output_key]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    168\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    169\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    154\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    155\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    156\u001b[0m     inputs,\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    161\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/agents/agent.py:987\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m--> 987\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[1;32m    988\u001b[0m         name_to_tool_map,\n\u001b[1;32m    989\u001b[0m         color_mapping,\n\u001b[1;32m    990\u001b[0m         inputs,\n\u001b[1;32m    991\u001b[0m         intermediate_steps,\n\u001b[1;32m    992\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[1;32m    993\u001b[0m     )\n\u001b[1;32m    994\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m    995\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[1;32m    996\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[1;32m    997\u001b[0m         )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/agents/agent.py:850\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    848\u001b[0m         tool_run_kwargs[\u001b[39m\"\u001b[39m\u001b[39mllm_prefix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    849\u001b[0m     \u001b[39m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[0;32m--> 850\u001b[0m     observation \u001b[39m=\u001b[39m tool\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    851\u001b[0m         agent_action\u001b[39m.\u001b[39;49mtool_input,\n\u001b[1;32m    852\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    853\u001b[0m         color\u001b[39m=\u001b[39;49mcolor,\n\u001b[1;32m    854\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtool_run_kwargs,\n\u001b[1;32m    856\u001b[0m     )\n\u001b[1;32m    857\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    858\u001b[0m     tool_run_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/tools/base.py:299\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    298\u001b[0m     run_manager\u001b[39m.\u001b[39mon_tool_error(e)\n\u001b[0;32m--> 299\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    300\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     run_manager\u001b[39m.\u001b[39mon_tool_end(\n\u001b[1;32m    302\u001b[0m         \u001b[39mstr\u001b[39m(observation), color\u001b[39m=\u001b[39mcolor, name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    303\u001b[0m     )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/tools/base.py:271\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     tool_args, tool_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[1;32m    270\u001b[0m     observation \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 271\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39m*\u001b[39;49mtool_args, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtool_kwargs)\n\u001b[1;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    273\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run(\u001b[39m*\u001b[39mtool_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtool_kwargs)\n\u001b[1;32m    274\u001b[0m     )\n\u001b[1;32m    275\u001b[0m \u001b[39mexcept\u001b[39;00m ToolException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    276\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_tool_error:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/tools/wikipedia/tool.py:31\u001b[0m, in \u001b[0;36mWikipediaQueryRun._run\u001b[0;34m(self, query, run_manager)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run\u001b[39m(\n\u001b[1;32m     26\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     27\u001b[0m     query: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     28\u001b[0m     run_manager: Optional[CallbackManagerForToolRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     29\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m     30\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Use the Wikipedia tool.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi_wrapper\u001b[39m.\u001b[39;49mrun(query)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/utilities/wikipedia.py:52\u001b[0m, in \u001b[0;36mWikipediaAPIWrapper.run\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m     51\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Run Wikipedia search and get page summaries.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     page_titles \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwiki_client\u001b[39m.\u001b[39;49msearch(query[:WIKIPEDIA_MAX_QUERY_LENGTH])\n\u001b[1;32m     53\u001b[0m     summaries \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m     \u001b[39mfor\u001b[39;00m page_title \u001b[39min\u001b[39;00m page_titles[: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtop_k_results]:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/wikipedia/util.py:28\u001b[0m, in \u001b[0;36mcache.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m   ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache[key]\n\u001b[1;32m     27\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m   ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache[key] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     30\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/wikipedia/wikipedia.py:103\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(query, results, suggestion)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m suggestion:\n\u001b[1;32m    101\u001b[0m   search_params[\u001b[39m'\u001b[39m\u001b[39msrinfo\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msuggestion\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 103\u001b[0m raw_results \u001b[39m=\u001b[39m _wiki_request(search_params)\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39merror\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m raw_results:\n\u001b[1;32m    106\u001b[0m   \u001b[39mif\u001b[39;00m raw_results[\u001b[39m'\u001b[39m\u001b[39merror\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39minfo\u001b[39m\u001b[39m'\u001b[39m] \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mHTTP request timed out.\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPool queue is full\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/wikipedia/wikipedia.py:737\u001b[0m, in \u001b[0;36m_wiki_request\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    734\u001b[0m   wait_time \u001b[39m=\u001b[39m (RATE_LIMIT_LAST_CALL \u001b[39m+\u001b[39m RATE_LIMIT_MIN_WAIT) \u001b[39m-\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m    735\u001b[0m   time\u001b[39m.\u001b[39msleep(\u001b[39mint\u001b[39m(wait_time\u001b[39m.\u001b[39mtotal_seconds()))\n\u001b[0;32m--> 737\u001b[0m r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(API_URL, params\u001b[39m=\u001b[39;49mparams, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    739\u001b[0m \u001b[39mif\u001b[39;00m RATE_LIMIT:\n\u001b[1;32m    740\u001b[0m   RATE_LIMIT_LAST_CALL \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/requests/adapters.py:501\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    503\u001b[0m \u001b[39mexcept\u001b[39;00m MaxRetryError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    504\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[1;32m    505\u001b[0m         \u001b[39m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[0;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))"
     ]
    }
   ],
   "source": [
    "agent.run(\"How much do data scientists make in America? How much if converted into Indonesia Rupiah?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Build Question Aswering System](#toc0_)\n",
    "\n",
    "### <a id='toc1_5_1_'></a>[SQLite Database](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain menyediakan fungsi yang menghubungkan basis data dengan LLM, yang disebut sebagai SQL Database. Ini juga menyediakan fungsi untuk menghubungkan antara basis data, model LLM, dan agen yang akan menjalankan kueri SQL berdasarkan prompt bahasa alami.\n",
    "\n",
    "Proses integrasi melibatkan pembuatan **koneksi** ke basis data, **mendefinisikan** kueri SQL yang diperlukan, dan **memanfaatkan** LLM dan agen untuk menjalankan kueri-kueri tersebut berdasarkan prompt pengguna. Ini memungkinkan interaksi dengan basis data secara ramah pengguna dan intuitif, dengan memanfaatkan kemampuan model bahasa untuk memahami dan memproses input bahasa alami."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk menggunakan LangChain untuk terhubung dengan basis data, kita perlu memanfaatkan kelas `SQLDatabase`. Kelas ini memungkinkan kita untuk membentuk koneksi antara LangChain dan basis data SQL, memungkinkan integrasi yang lancar antara kueri bahasa alami dan eksekusi perintah SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SQLDatabase, SQLDatabaseChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada bagian ini, kita perlu memuat data. Sebagai contoh, kita akan menggunakan data chinook dari kelas akademi kita. Anda perlu menjelaskan secara eksplisit jenis basis data yang Anda muat, misalnya `sqlite:///`.\n",
    "\n",
    "Kemudian kita dapat memuat basis data dengan menggunakan `SQLDatabase` dari `langchain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dburi = \"sqlite:///data_input/chinook.db\"\n",
    "db = SQLDatabase.from_uri(dburi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `from_uri()` memungkinkan kita untuk berinteraksi dengan basis data dan melakukan berbagai operasi seperti melakukan kueri data.\n",
    "\n",
    "Setelah itu, kita menghubungkan `db` ke model, menciptakan agen yang dapat mencari jawaban dalam basis data Chinook berdasarkan masukan prompt.\n",
    "\n",
    "Mari coba sebuah prompt untuk mengetahui berapa banyak baris yang ada di tabel \"tracks\" dalam basis data ini.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0) # parameter temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/chains/sql_database/base.py:63: UserWarning: Directly instantiating an SQLDatabaseChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "How many rows is in the tracks table of this db?\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT COUNT(*) FROM tracks;\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(3503,)]\u001b[0m\n",
      "Answer:\u001b[32;1m\u001b[1;3mThe tracks table has 3503 rows.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The tracks table has 3503 rows.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)\n",
    "\n",
    "db_chain.run(\"How many rows is in the tracks table of this db?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhatikan bahwa keluaran mengandung beberapa komponen:\n",
    "- `SQLQuery`, yang memberikan informasi tentang proses yang digunakan oleh model untuk mencari jawaban menggunakan SQL.\n",
    "- `SQLResult`, yang mewakili hasil yang diperoleh dari menjalankan kueri SQL pada basis data kita.\n",
    "- `Answer`, yang mengonversi `SQLResult` menjadi bahasa alami dan menampilkannya sebagai jawaban akhir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_5_1_1_'></a>[Basic Question Answer System](#toc0_)\n",
    "\n",
    "Contoh:\n",
    "\n",
    "> all sales in rock genre in 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "all sales in rock genre in 2012 based on invoice\n",
      "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT i.InvoiceId, i.InvoiceDate, i.Total FROM invoices i \n",
      "INNER JOIN invoice_items ii ON i.InvoiceId = ii.InvoiceId \n",
      "INNER JOIN tracks t ON ii.TrackId = t.TrackId \n",
      "INNER JOIN genres g ON t.GenreId = g.GenreId \n",
      "WHERE g.Name = 'Rock' AND strftime('%Y', i.InvoiceDate) = '2012'\n",
      "LIMIT 5;\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(319, '2012-11-01 00:00:00', 8.91), (319, '2012-11-01 00:00:00', 8.91), (319, '2012-11-01 00:00:00', 8.91), (319, '2012-11-01 00:00:00', 8.91), (320, '2012-11-06 00:00:00', 13.86)]\u001b[0m\n",
      "Answer:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThe sales in rock genre in 2012 based on invoice are InvoiceId 319 with a total of 8.91, InvoiceId 319 with a total of 8.91, InvoiceId 319 with a total of 8.91, InvoiceId 319 with a total of 8.91, and InvoiceId 320 with a total of 13.86.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The sales in rock genre in 2012 based on invoice are InvoiceId 319 with a total of 8.91, InvoiceId 319 with a total of 8.91, InvoiceId 319 with a total of 8.91, InvoiceId 319 with a total of 8.91, and InvoiceId 320 with a total of 13.86.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_chain.run(\"all sales in rock genre in 2012 based on invoice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "We want the returned DataFrame to contain only the Pop genre and only when the UnitPrice of the track is 0.99\n",
      "SQLQuery:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mSELECT * FROM tracks WHERE GenreId = 3 AND UnitPrice = 0.99 LIMIT 5;\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[(77, 'Enter Sandman', 9, 1, 3, 'Apocalyptica', 221701, 7286305, 0.99), (78, 'Master Of Puppets', 9, 1, 3, 'Apocalyptica', 436453, 14375310, 0.99), (79, 'Harvester Of Sorrow', 9, 1, 3, 'Apocalyptica', 374543, 12372536, 0.99), (80, 'The Unforgiven', 9, 1, 3, 'Apocalyptica', 322925, 10422447, 0.99), (81, 'Sad But True', 9, 1, 3, 'Apocalyptica', 288208, 9405526, 0.99)]\u001b[0m\n",
      "Answer:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThe DataFrame contains 5 tracks from the Pop genre with a UnitPrice of 0.99.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The DataFrame contains 5 tracks from the Pop genre with a UnitPrice of 0.99.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_chain.run(\"We want the returned DataFrame to contain only the Pop genre and only when the UnitPrice of the track is 0.99\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Tampilkan lagu dengan Genre Pop\n",
      "SQLQuery:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mSELECT \"Name\" FROM tracks WHERE \"GenreId\" = 3 LIMIT 5;\u001b[0m\n",
      "SQLResult: \u001b[33;1m\u001b[1;3m[('Enter Sandman',), ('Master Of Puppets',), ('Harvester Of Sorrow',), ('The Unforgiven',), ('Sad But True',)]\u001b[0m\n",
      "Answer:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mLagu dengan Genre Pop adalah Enter Sandman, Master Of Puppets, Harvester Of Sorrow, The Unforgiven, dan Sad But True.\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Lagu dengan Genre Pop adalah Enter Sandman, Master Of Puppets, Harvester Of Sorrow, The Unforgiven, dan Sad But True.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_chain.run(\"Tampilkan lagu dengan Genre Pop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_2_'></a>[Structured Data](#toc0_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"data_input/rice.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya, kita akan membuat sebuah agen yang secara khusus dirancang untuk bekerja dengan data CSV. Agen ini akan memungkinkan kita **untuk melakukan kueri dan mengambil informasi dari dataset `rice.csv`**. Karena kita menggunakan model LLM yang sama seperti dalam bagian SQL, tidak perlu mendefinisikan ulang LLM. Kita dapat memanfaatkan model LLM yang sudah ada untuk agen CSV kita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_csv_agent\n",
    "agent = create_csv_agent(llm, filepath, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coba kita tanya beberapa pertanyaan tentang data rice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: saya harus menghitung jumlah transaksi yang terjadi di setiap format\n",
      "Action: python_repl_ast\n",
      "Action Input: df.groupby('format')['receipt_id'].count()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mformat\n",
      "hypermarket     999\n",
      "minimarket     7088\n",
      "supermarket    3913\n",
      "Name: receipt_id, dtype: int64\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m Saya sekarang tahu jawaban akhir\n",
      "Final Answer: Hypermarket memiliki 999 transaksi, Minimarket memiliki 7088 transaksi, dan Supermarket memiliki 3913 transaksi.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hypermarket memiliki 999 transaksi, Minimarket memiliki 7088 transaksi, dan Supermarket memiliki 3913 transaksi.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"berikan detail banyaknya transaksi yang terjadi di setiap format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhatikan bahwa terdapat komponen tambahan dalam output:\n",
    "- `Thought`: Ini mewakili **pemikiran agen** dalam memecahkan masalah berdasarkan prompt yang diberikan. Ini memberikan wawasan tentang proses pengambilan keputusan agen dan alasan di balik tindakan-tindakannya.\n",
    "- `Action`: Ini menggambarkan **tindakan-tindakan yang diambil** oleh agen untuk memecahkan masalah. Dalam hal ini, melibatkan penggunaan alat `python_repl_ast`, yang merupakan shell Python. Ini juga menunjukkan perintah `pandas` yang spesifik yang digunakan oleh agen untuk mengekstrak hasil dari data CSV.\n",
    "- `Final Answer`: Ini adalah representasi bahasa alami dari **jawaban** yang diperoleh dari hasil dari `Input Tindakan`. Ini menyajikan respons akhir terhadap prompt dalam format yang dapat dibaca manusia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_3_'></a>[Unstructured Data](#toc0_)\n",
    "\n",
    "Untuk memulai, mari tentukan jalur file kumpulan data `universal_studio_branches.csv` kami, yang berisi sekumpulan ulasan dari pengunjung Universal Studios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'data_input/universal_studio_branches.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya, kita akan membuat agen yang dirancang khusus untuk bekerja dengan data CSV. Agen ini akan memungkinkan kita untuk menanyakan dan mengambil informasi dari kumpulan data `universal_studio_branches.csv`. Karena kita menggunakan model LLM yang sama seperti pada bagian SQL, LLM tidak perlu didefinisikan ulang. Kita bisa memanfaatkan model LLM yang ada untuk agen CSV kita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_csv_agent\n",
    "agent = create_csv_agent(llm, datapath, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to know the size of the dataframe\n",
      "Action: python_repl_ast\n",
      "Action Input: df.shape\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m(50904, 6)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 50,904 review records are in the dataset.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'50,904 review records are in the dataset.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"How many review records are in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to know what the dataframe contains\n",
      "Action: python_repl_ast\n",
      "Action Input: df.info()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50904 entries, 0 to 50903\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   reviewer      50904 non-null  object \n",
      " 1   rating        50904 non-null  float64\n",
      " 2   written_date  50904 non-null  object \n",
      " 3   title         50904 non-null  object \n",
      " 4   review_text   50904 non-null  object \n",
      " 5   branch        50904 non-null  object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 2.3+ MB\n",
      "\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I now know the details of the dataset\n",
      "Final Answer: The dataframe contains 50904 entries, 6 columns, and the columns are reviewer, rating, written_date, title, review_text, and branch. The data types are float64 (1), and object (5).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The dataframe contains 50904 entries, 6 columns, and the columns are reviewer, rating, written_date, title, review_text, and branch. The data types are float64 (1), and object (5).'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Give me a detail info about the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to calculate the percentage of positive reviews\n",
      "Action: python_repl_ast\n",
      "Action Input: df[df['rating'] > 3].shape[0] / df.shape[0]\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m0.8195033789093195\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-zswZwidekCnb9bg9VvzfjN6p on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 80.95% of the reviews in this dataset are positive.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'80.95% of the reviews in this dataset are positive.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"How many percent a positive review from this datasets?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to calculate the percentage of negative reviews\n",
      "Action: python_repl_ast\n",
      "Action Input: df[df['rating'] < 3]['rating'].count() / df['rating'].count() * 100\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m7.777384881345277\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: 7.78% of the reviews are negative.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'7.78% of the reviews are negative.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"How many percent a negative review from this datasets?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_5_3_1_'></a>[Visualization](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to group the data by branch and rating\n",
      "Action: python_repl_ast\n",
      "Action Input: df.groupby(['branch', 'rating']).size().unstack().plot(kind='bar', stacked=True)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAxes(0.125,0.11;0.775x0.77)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The plot shows the count of each rating per branch.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The plot shows the count of each rating per branch.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.run(\"see how count of each rating using plot per branch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agent\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mHow long is the longest line of visitors for Universal Studio rides?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/chains/base.py:290\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, *args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    289\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 290\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags)[_output_key]\n\u001b[1;32m    292\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags)[_output_key]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    168\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    169\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    154\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    155\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    156\u001b[0m     inputs,\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    161\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/agents/agent.py:987\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m--> 987\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[1;32m    988\u001b[0m         name_to_tool_map,\n\u001b[1;32m    989\u001b[0m         color_mapping,\n\u001b[1;32m    990\u001b[0m         inputs,\n\u001b[1;32m    991\u001b[0m         intermediate_steps,\n\u001b[1;32m    992\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[1;32m    993\u001b[0m     )\n\u001b[1;32m    994\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m    995\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[1;32m    996\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[1;32m    997\u001b[0m         )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/agents/agent.py:792\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Take a single step in the thought-action-observation loop.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \n\u001b[1;32m    788\u001b[0m \u001b[39mOverride this to take control of how the agent makes and acts on choices.\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m     \u001b[39m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 792\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mplan(\n\u001b[1;32m    793\u001b[0m         intermediate_steps,\n\u001b[1;32m    794\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    795\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs,\n\u001b[1;32m    796\u001b[0m     )\n\u001b[1;32m    797\u001b[0m \u001b[39mexcept\u001b[39;00m OutputParserException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    798\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_parsing_errors, \u001b[39mbool\u001b[39m):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/agents/agent.py:443\u001b[0m, in \u001b[0;36mAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39m    Action specifying what tool to use.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    442\u001b[0m full_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 443\u001b[0m full_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_chain\u001b[39m.\u001b[39;49mpredict(callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfull_inputs)\n\u001b[1;32m    444\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_parser\u001b[39m.\u001b[39mparse(full_output)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/chains/llm.py:252\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    238\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \n\u001b[1;32m    240\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/chains/base.py:166\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    167\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    168\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    169\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    170\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, include_run_info)\u001b[0m\n\u001b[1;32m    154\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    155\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    156\u001b[0m     inputs,\n\u001b[1;32m    157\u001b[0m )\n\u001b[1;32m    158\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    161\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    165\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/chains/llm.py:92\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     88\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     89\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     90\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     91\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 92\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m     93\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/chains/llm.py:102\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    103\u001b[0m     prompts,\n\u001b[1;32m    104\u001b[0m     stop,\n\u001b[1;32m    105\u001b[0m     callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    106\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_kwargs,\n\u001b[1;32m    107\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/llms/base.py:139\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    133\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    137\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    138\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/llms/base.py:225\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    220\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    221\u001b[0m         )\n\u001b[1;32m    222\u001b[0m     run_managers \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    223\u001b[0m         dumpd(\u001b[39mself\u001b[39m), prompts, invocation_params\u001b[39m=\u001b[39mparams, options\u001b[39m=\u001b[39moptions\n\u001b[1;32m    224\u001b[0m     )\n\u001b[0;32m--> 225\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    226\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    227\u001b[0m     )\n\u001b[1;32m    228\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/llms/base.py:176\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    175\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 176\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    177\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    178\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/llms/base.py:163\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    154\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    155\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    160\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    161\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 163\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    164\u001b[0m                 prompts,\n\u001b[1;32m    165\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    166\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    167\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    168\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    169\u001b[0m             )\n\u001b[1;32m    170\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    172\u001b[0m         )\n\u001b[1;32m    173\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    174\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/llms/openai.py:336\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     response \u001b[39m=\u001b[39m completion_with_retry(\u001b[39mself\u001b[39;49m, prompt\u001b[39m=\u001b[39;49m_prompts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    337\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    338\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstreaming:\n\u001b[1;32m    339\u001b[0m     \u001b[39m# Can't update token usage if streaming\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/llms/openai.py:106\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[0;34m(llm, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    104\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 106\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/langchain/llms/openai.py:104\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/openai/api_requestor.py:288\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[1;32m    291\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    292\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    293\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    294\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    295\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[1;32m    298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/openai/api_requestor.py:596\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m     _thread_context\u001b[39m.\u001b[39msession_create_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    595\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    597\u001b[0m         method,\n\u001b[1;32m    598\u001b[0m         abs_url,\n\u001b[1;32m    599\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    600\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    601\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    602\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    603\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[1;32m    604\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    607\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    462\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1376\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/llm_dev/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.run(\"How long is the longest line of visitors for Universal Studio rides?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
